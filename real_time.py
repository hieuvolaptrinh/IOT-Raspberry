"""
REAL-TIME VSL CLIENT FOR RASPBERRY PI
Connects to backend WebSocket, streams audio, receives VSL text, and plays local videos.

Features:
- WebSocket connection to backend
- Audio streaming from USB microphone
- Local video playback from video/ folder
- LCD display for visual feedback
"""

import cv2
import numpy as np
import spidev
import RPi.GPIO as GPIO
import time
import os
import subprocess
import asyncio
import websockets
import json
import threading
import re
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv
from PIL import Image, ImageDraw, ImageFont

# ============ LOAD .ENV ============
load_dotenv()
_api_url = os.getenv("API_URL", "ws://172.20.10.3:8000")

# Auto-convert http:// to ws:// and https:// to wss://
if _api_url.startswith("http://"):
    API_URL = _api_url.replace("http://", "ws://", 1)
elif _api_url.startswith("https://"):
    API_URL = _api_url.replace("https://", "wss://", 1)
else:
    API_URL = _api_url

WS_ENDPOINT = "/api/realtime/ws/vsl"

# ============ CONNECTION SETTINGS ============
RECONNECT_DELAY = 3  # Seconds between reconnect attempts
MAX_RECONNECT_ATTEMPTS = 5
SILENCE_THRESHOLD = 500  # RMS threshold for silence detection
SILENCE_DURATION = 1.5  # Seconds of silence before flushing

# ============ DISPLAY SETTINGS ============
MIRROR_MODE = True  # Set True for VR glasses (flip horizontal)
SKIP_FRAMES = 1  # Skip every N frames for faster playback (1 = no skip)

# ============ GPIO PINS ============
BUTTON_PIN = 17  # Pin 11
DC_PIN = 24
RST_PIN = 25
BL_PIN = 18

# ============ PATHS ============
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
VIDEO_DIR = os.path.join(SCRIPT_DIR, "video")
FONT_PATH = os.path.join(SCRIPT_DIR, "SVN-Arial Regular.ttf")

# ============ AUDIO SETTINGS ============
SAMPLE_RATE = 16000  # Whisper expects 16kHz
CHANNELS = 1
CHUNK_SIZE = 8000  # 0.5 seconds of audio at 16kHz

# ============ STATE ============
class State:
    IDLE = 0
    CONNECTING = 1
    RECORDING = 2  # Actively recording and sending audio
    PROCESSING = 3
    PLAYING = 4

current_state = State.IDLE
is_recording = False  # Toggle for recording mode
stop_streaming = False
stop_video = False
websocket_connected = False
reconnect_count = 0
last_audio_time = 0  # For silence detection
ws_thread = None  # WebSocket thread reference


# ============ AUDIO DEVICE ============
def get_usb_audio_device():
    """Auto-detect USB audio device."""
    try:
        result = subprocess.run(['arecord', '-l'], capture_output=True, text=True)
        lines = result.stdout.split('\n')
        
        for line in lines:
            if 'card' in line.lower() and ('usb' in line.lower() or 'pnp' in line.lower()):
                match = re.search(r'card (\d+):', line)
                if match:
                    card_num = match.group(1)
                    device = f"plughw:{card_num},0"
                    print(f"üé§ Found USB Audio: {line.strip()}")
                    print(f"   ‚Üí Using: {device}")
                    return device
        
        print("‚ö†Ô∏è USB Audio not found, using default: plughw:0,0")
        return "plughw:0,0"
    except Exception as e:
        print(f"‚ö†Ô∏è Error finding audio device: {e}")
        return "plughw:0,0"


AUDIO_DEVICE = get_usb_audio_device()


# ============ FONT ============
try:
    FONT_VN = ImageFont.truetype(FONT_PATH, 22)  # Bigger font
    FONT_SMALL = ImageFont.truetype(FONT_PATH, 16)
    FONT_LARGE = ImageFont.truetype(FONT_PATH, 28)  # For titles
except:
    FONT_VN = ImageFont.load_default()
    FONT_SMALL = ImageFont.load_default()
    FONT_LARGE = ImageFont.load_default()
    print("‚ö†Ô∏è Font not found, using default")


# ============ GPIO + SPI SETUP ============
try:
    GPIO.cleanup()
except:
    pass

GPIO.setmode(GPIO.BCM)
GPIO.setwarnings(False)
GPIO.setup(BUTTON_PIN, GPIO.IN, pull_up_down=GPIO.PUD_UP)
GPIO.setup(DC_PIN, GPIO.OUT)
GPIO.setup(RST_PIN, GPIO.OUT)
GPIO.setup(BL_PIN, GPIO.OUT)
GPIO.output(BL_PIN, GPIO.HIGH)

spi = spidev.SpiDev()
spi.open(0, 0)
spi.max_speed_hz = 32000000
spi.mode = 3


# ============ LCD FUNCTIONS ============
def cmd(c):
    GPIO.output(DC_PIN, GPIO.LOW)
    spi.xfer2([c])


def data(d):
    GPIO.output(DC_PIN, GPIO.HIGH)
    if isinstance(d, list):
        spi.xfer2(d)
    else:
        spi.xfer2([d])


def data_bulk(d):
    """Send bulk data to LCD - ULTRA OPTIMIZED with writebytes2."""
    GPIO.output(DC_PIN, GPIO.HIGH)
    # writebytes2 is faster than xfer2 for large data (no return buffer)
    # Use larger chunks for fewer calls
    CHUNK_SIZE = 32768  # 32KB chunks (was 4KB)
    if isinstance(d, bytes):
        for i in range(0, len(d), CHUNK_SIZE):
            spi.writebytes2(d[i:i+CHUNK_SIZE])
    else:
        # Convert to bytes first if it's a list
        d_bytes = bytes(d)
        for i in range(0, len(d_bytes), CHUNK_SIZE):
            spi.writebytes2(d_bytes[i:i+CHUNK_SIZE])


def init_lcd():
    GPIO.output(RST_PIN, GPIO.HIGH)
    time.sleep(0.05)
    GPIO.output(RST_PIN, GPIO.LOW)
    time.sleep(0.05)
    GPIO.output(RST_PIN, GPIO.HIGH)
    time.sleep(0.15)
    
    cmd(0x01); time.sleep(0.15)
    cmd(0x11); time.sleep(0.12)
    cmd(0x36); data(0x08)
    cmd(0x3A); data(0x55)
    cmd(0xB2); data([0x0C, 0x0C, 0x00, 0x33, 0x33])
    cmd(0xB7); data(0x35)
    cmd(0xBB); data(0x28)
    cmd(0xC0); data(0x0C)
    cmd(0xC2); data([0x01, 0xFF])
    cmd(0xC3); data(0x10)
    cmd(0xC4); data(0x20)
    cmd(0xC6); data(0x0F)
    cmd(0xD0); data([0xA4, 0xA1])
    cmd(0x21)
    cmd(0x13); time.sleep(0.01)
    cmd(0x29); time.sleep(0.12)


# Pre-allocate display buffer for performance
_display_buffer = np.empty((240, 240, 2), dtype=np.uint8)
_rgb565_buffer = np.empty((240, 240), dtype=np.uint16)


def show_frame(frame, overlay_text=None):
    """Display a frame on LCD - supports Vietnamese font and mirror mode."""
    global _display_buffer, _rgb565_buffer
    
    # Resize with fastest interpolation
    frame = cv2.resize(frame, (240, 240), interpolation=cv2.INTER_NEAREST)
    
    # Add text overlay BEFORE mirroring (so text is mirrored too)
    if overlay_text:
        # Convert to PIL for Vietnamese font support
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        pil_img = Image.fromarray(frame_rgb)
        draw = ImageDraw.Draw(pil_img)
        
        # Black bar at bottom
        draw.rectangle([(0, 200), (240, 240)], fill=(0, 0, 0))
        
        # White text, centered
        text = overlay_text[:30]
        try:
            bbox = draw.textbbox((0, 0), text, font=FONT_VN)
            text_width = bbox[2] - bbox[0]
        except:
            text_width = len(text) * 10
        x = max(5, (240 - text_width) // 2)
        draw.text((x, 210), text, font=FONT_VN, fill=(255, 255, 255))
        
        frame = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)
    
    # Mirror for VR glasses AFTER adding text (so text is mirrored too)
    if MIRROR_MODE:
        frame = cv2.flip(frame, 1)
    
    # OPTIMIZED RGB565 conversion
    np.add(
        np.add(
            np.left_shift(frame[:, :, 2].astype(np.uint16) >> 3, 11),
            np.left_shift(frame[:, :, 1].astype(np.uint16) >> 2, 5)
        ),
        frame[:, :, 0].astype(np.uint16) >> 3,
        out=_rgb565_buffer
    )
    
    # Split into high/low bytes
    _display_buffer[:, :, 0] = (_rgb565_buffer >> 8).astype(np.uint8)
    _display_buffer[:, :, 1] = (_rgb565_buffer & 0xFF).astype(np.uint8)
    
    # Send to LCD
    cmd(0x2A); data([0, 0, 0, 239])
    cmd(0x2B); data([0, 0, 0, 239])
    cmd(0x2C)
    data_bulk(_display_buffer.tobytes())


def show_message(lines, color=(255, 255, 255), bg_color=(0, 0, 0)):
    """Display text message on LCD with white/bright colors."""
    pil_img = Image.new('RGB', (240, 240), bg_color)
    draw = ImageDraw.Draw(pil_img)
    
    if isinstance(lines, str):
        lines = lines.split('\n')
    
    total_height = len(lines) * 35  # Increased line height
    start_y = (240 - total_height) // 2
    
    for i, line in enumerate(lines):
        # Get text width for centering
        bbox = draw.textbbox((0, 0), line, font=FONT_VN)
        text_width = bbox[2] - bbox[0]
        x = max(5, (240 - text_width) // 2)
        y = start_y + i * 35
        draw.text((x, y), line, font=FONT_VN, fill=color)
    
    frame = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)
    show_frame(frame)
    
    # Free memory
    del pil_img, draw, frame


# ============ VIDEO MAPPER ============
class VideoMapper:
    """Map words to local video files."""
    
    # Windows reserved names
    RESERVED_NAMES = {'con', 'prn', 'aux', 'nul', 'com1', 'com2', 'lpt1'}
    
    # Vietnamese tone marks ‚Üí base letter
    TONE_MAP = {
        '√†': 'a', '√°': 'a', '·∫£': 'a', '√£': 'a', '·∫°': 'a',
        'ƒÉ': 'ƒÉ', '·∫±': 'ƒÉ', '·∫Ø': 'ƒÉ', '·∫≥': 'ƒÉ', '·∫µ': 'ƒÉ', '·∫∑': 'ƒÉ',
        '√¢': '√¢', '·∫ß': '√¢', '·∫•': '√¢', '·∫©': '√¢', '·∫´': '√¢', '·∫≠': '√¢',
        '√®': 'e', '√©': 'e', '·∫ª': 'e', '·∫Ω': 'e', '·∫π': 'e',
        '√™': '√™', '·ªÅ': '√™', '·∫ø': '√™', '·ªÉ': '√™', '·ªÖ': '√™', '·ªá': '√™',
        '√¨': 'i', '√≠': 'i', '·ªâ': 'i', 'ƒ©': 'i', '·ªã': 'i',
        '√≤': 'o', '√≥': 'o', '·ªè': 'o', '√µ': 'o', '·ªç': 'o',
        '√¥': '√¥', '·ªì': '√¥', '·ªë': '√¥', '·ªï': '√¥', '·ªó': '√¥', '·ªô': '√¥',
        '∆°': '∆°', '·ªù': '∆°', '·ªõ': '∆°', '·ªü': '∆°', '·ª°': '∆°', '·ª£': '∆°',
        '√π': 'u', '√∫': 'u', '·ªß': 'u', '≈©': 'u', '·ª•': 'u',
        '∆∞': '∆∞', '·ª´': '∆∞', '·ª©': '∆∞', '·ª≠': '∆∞', '·ªØ': '∆∞', '·ª±': '∆∞',
        '·ª≥': 'y', '√Ω': 'y', '·ª∑': 'y', '·ªπ': 'y', '·ªµ': 'y',
    }
    
    def __init__(self, video_dir: str):
        self.video_dir = Path(video_dir)
        self.video_cache = {}
        self._scan_videos()
        print(f"üìπ VideoMapper: {len(self.video_cache)} videos loaded")
    
    def _scan_videos(self):
        """Scan and cache all video files."""
        if not self.video_dir.exists():
            print(f"‚ö†Ô∏è Video dir not found: {self.video_dir}")
            return
        
        for f in self.video_dir.glob("*.mp4"):
            key = f.stem.lower()
            self.video_cache[key] = f
        
        for f in self.video_dir.glob("*.webm"):
            key = f.stem.lower()
            if key not in self.video_cache:
                self.video_cache[key] = f
    
    def normalize_for_pronunciation(self, text: str) -> str:
        """Remove tone marks for fuzzy matching."""
        result = []
        for char in text.lower():
            result.append(self.TONE_MAP.get(char, char))
        return ''.join(result)
    
    def find_video(self, word: str) -> Path:
        """Find video file for a word."""
        if not word:
            return None
        
        key = word.lower().strip()
        
        # Strategy 1: Exact match
        if key in self.video_cache:
            return self.video_cache[key]
        
        # Strategy 2: Windows reserved names (con ‚Üí con_)
        if key in self.RESERVED_NAMES:
            key_reserved = key + '_'
            if key_reserved in self.video_cache:
                return self.video_cache[key_reserved]
        
        # Strategy 3: Underscore format
        key_underscore = key.replace(' ', '_')
        if key_underscore in self.video_cache:
            return self.video_cache[key_underscore]
        
        # Strategy 4: Remove tone marks
        key_no_tone = self.normalize_for_pronunciation(key)
        if key_no_tone in self.video_cache:
            return self.video_cache[key_no_tone]
        
        return None
    
    def get_fingerspell_videos(self, word: str) -> list:
        """Get list of videos for fingerspelling a word."""
        result = []
        for char in word.lower():
            if char.isalpha():
                normalized = self.TONE_MAP.get(char, char)
                video = self.find_video(normalized)
                if video:
                    result.append((normalized, video))
            elif char.isdigit():
                video = self.find_video(char)
                if video:
                    result.append((char, video))
        return result


# Initialize video mapper
video_mapper = VideoMapper(VIDEO_DIR)


# ============ VIDEO PLAYBACK (THREADED) ============
# Queue for video playback tasks
import queue
video_queue = queue.Queue()
video_thread_running = True


def video_playback_worker():
    """Worker thread that plays videos from queue - doesn't block WebSocket."""
    global video_thread_running, current_state, stop_video
    
    while video_thread_running:
        try:
            # Wait for video task with timeout (to check running flag)
            task = video_queue.get(timeout=0.5)
            if task is None:
                continue
            
            words, transcript = task
            current_state = State.PLAYING
            stop_video = False
            
            print(f"Playing sequence: {words}")
            
            for word in words:
                if stop_video:
                    break
                
                video_path = video_mapper.find_video(word)
                
                if video_path:
                    print(f"   > {word} -> {video_path.name}")
                    play_single_video(str(video_path), transcript)
                else:
                    # Fingerspell fallback
                    print(f"   Fingerspelling: {word}")
                    letters = video_mapper.get_fingerspell_videos(word)
                    for letter, letter_video in letters:
                        if stop_video:
                            break
                        play_single_video(str(letter_video), transcript, duration=0.4)
            
            current_state = State.RECORDING
            stop_video = False
            video_queue.task_done()
            
        except queue.Empty:
            continue
        except Exception as e:
            print(f"Video worker error: {e}")


def play_video_sequence(words: list, transcript: str = ""):
    """Queue video sequence for playback in background thread."""
    video_queue.put((words, transcript))


# Start video worker thread
video_thread = threading.Thread(target=video_playback_worker, daemon=True)
video_thread.start()
print("Video worker thread started")


def play_single_video(video_path: str, overlay_word: str = "", duration: float = None):
    """Play a single video file on LCD - FULL PLAYBACK with debug info."""
    global stop_video
    
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"Cannot open: {video_path}")
        return

    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
    
    # Get video info
    fps = cap.get(cv2.CAP_PROP_FPS) or 25
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    video_duration = total_frames / fps if fps > 0 else 0
    
    print(f"   [Video: {total_frames} frames, {fps:.1f}fps, {video_duration:.2f}s]")
    
    # Full speed playback
    frame_delay = 1.0 / fps
    
    frame_count = 0
    gc_interval = 20
    
    try:
        while not stop_video:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_count += 1
            
            # Display every frame
            show_frame(frame, overlay_word)
            del frame

            # Periodic GC
            if frame_count % gc_interval == 0:
                import gc
                gc.collect()

            # Only check duration limit for fingerspell
            if duration and frame_count >= int(duration * fps):
                break
            
            time.sleep(frame_delay)
        
        print(f"   [Played: {frame_count}/{total_frames} frames]")
            
    except Exception as e:
        print(f"Video playback error: {e}")
    finally:
        cap.release()
        import gc
        gc.collect()


# ============ WEBSOCKET CLIENT ============
async def stream_audio_to_server(ws):
    """Stream audio chunks to WebSocket server - memory optimized."""
    global stop_streaming
    
    print("üé§ Starting audio stream...")
    
    # Start arecord process with smaller buffer
    process = subprocess.Popen([
        'arecord', '-D', AUDIO_DEVICE,
        '-f', 'S16_LE', '-r', str(SAMPLE_RATE),
        '-c', str(CHANNELS), '-t', 'raw', '-'
    ], stdout=subprocess.PIPE, stderr=subprocess.DEVNULL, bufsize=CHUNK_SIZE * 2)
    
    try:
        while not stop_streaming:
            # Read audio chunk
            audio_data = process.stdout.read(CHUNK_SIZE * 2)  # 16-bit = 2 bytes
            
            if not audio_data:
                break
            
            # Send to server immediately
            await ws.send(audio_data)
            
            # Delete audio data immediately after sending to free memory
            del audio_data
            
            await asyncio.sleep(0.005)  # Minimal delay
    finally:
        process.terminate()
        process.wait()
        print("üé§ Audio stream stopped")


async def receive_results(ws):
    """Receive and process results from server."""
    global current_state, stop_streaming
    
    try:
        async for message in ws:
            data = json.loads(message)
            msg_type = data.get('type', '')
            
            if msg_type == 'connected':
                print(f"Connected: {data.get('message')}")
                show_message(["Da ket noi!", "", "Dang lang nghe..."], (100, 255, 100))
            
            elif msg_type == 'buffering':
                progress = data.get('progress', 0)
                print(f"   Buffering: {progress*100:.0f}%")
            
            elif msg_type == 'result':
                transcript = data.get('transcript', '')
                vsl_text = data.get('vsl_text', '')
                words = data.get('words', [])
                confidence = data.get('confidence', 0)
                
                print(f"Transcript: {transcript}")
                print(f"VSL Text: {vsl_text}")
                print(f"Confidence: {confidence:.2f}")
                
                if words:
                    play_video_sequence(words, transcript)
            
            elif msg_type == 'error':
                print(f"Error: {data.get('error')}")
                show_message(["Loi!", data.get('error', '')[:20]], (255, 100, 100))
            
            # Free message memory after processing
            del data, message
    
    except Exception as e:
        print(f"Receive error: {e}")


async def websocket_session():
    """Main WebSocket session with auto-reconnect."""
    global current_state, stop_streaming, websocket_connected, reconnect_count, is_recording
    
    ws_url = f"{API_URL}{WS_ENDPOINT}"
    print(f"üîå Connecting to: {ws_url}")
    
    try:
        async with websockets.connect(
            ws_url,
            ping_interval=30,   # Send ping every 30s
            ping_timeout=60,    # Wait 60s for pong (allow long video playback)
            close_timeout=10
        ) as ws:
            websocket_connected = True
            current_state = State.RECORDING
            reconnect_count = 0  # Reset on successful connection
            
            # Show recording status on LCD
            show_message([
                "üî¥ ƒêANG GHI √ÇM",
                "",
                "N√≥i v√†o micro...",
                "Nh·∫•n n√∫t ƒë·ªÉ d·ª´ng"
            ], (255, 100, 100), (50, 0, 0))
            
            # Run sender and receiver concurrently
            sender = asyncio.create_task(stream_audio_to_server(ws))
            receiver = asyncio.create_task(receive_results(ws))
            heartbeat = asyncio.create_task(send_heartbeat(ws))
            
            # Wait until stop_streaming is set or connection closes
            # Use ALL_COMPLETED so session stays alive while any task runs
            try:
                await asyncio.gather(sender, receiver, heartbeat)
            except Exception as e:
                print(f"Task error: {e}")
    
    except websockets.exceptions.ConnectionClosed:
        print("üîå Connection closed")
    except Exception as e:
        print(f"‚ùå Connection error: {e}")
        show_message(["L·ªói k·∫øt n·ªëi!", str(e)[:20]], (255, 100, 100))
    finally:
        websocket_connected = False
        current_state = State.IDLE


async def send_heartbeat(ws):
    """Send periodic heartbeat to keep connection alive."""
    try:
        while not stop_streaming:
            await asyncio.sleep(15)
            if websocket_connected:
                await ws.send(json.dumps({'type': 'ping'}))
    except:
        pass


async def websocket_session_with_reconnect():
    """Wrapper with auto-reconnect logic."""
    global reconnect_count, stop_streaming
    
    while not stop_streaming and reconnect_count < MAX_RECONNECT_ATTEMPTS:
        await websocket_session()
        
        if stop_streaming:
            break
        
        reconnect_count += 1
        if reconnect_count < MAX_RECONNECT_ATTEMPTS:
            print(f"üîÑ Reconnecting in {RECONNECT_DELAY}s... (attempt {reconnect_count}/{MAX_RECONNECT_ATTEMPTS})")
            show_message(["M·∫•t k·∫øt n·ªëi", f"Th·ª≠ l·∫°i {reconnect_count}/{MAX_RECONNECT_ATTEMPTS}"], (255, 200, 100))
            await asyncio.sleep(RECONNECT_DELAY)
        else:
            print("‚ùå Max reconnect attempts reached")
            show_message(["Kh√¥ng th·ªÉ k·∫øt n·ªëi", "Nh·∫•n n√∫t ƒë·ªÉ th·ª≠ l·∫°i"], (255, 100, 100))


def start_websocket_thread():
    """Start WebSocket session in a separate thread."""
    global stop_streaming, reconnect_count
    stop_streaming = False
    reconnect_count = 0
    
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    
    try:
        loop.run_until_complete(websocket_session_with_reconnect())
    finally:
        loop.close()


# ============ BUTTON HANDLER ============
def handle_button():
    """Handle button press - Toggle recording on/off."""
    global current_state, is_recording, stop_streaming, stop_video, ws_thread
    
    print(f"üîò Button pressed! State: {current_state}, Recording: {is_recording}")
    
    if current_state == State.PLAYING:
        # Stop video playback first
        print("‚èπ Stopping video...")
        stop_video = True
        return
    
    if not is_recording:
        # ===== START RECORDING =====
        print("üî¥ Starting recording...")
        is_recording = True
        stop_streaming = False
        current_state = State.CONNECTING
        
        show_message([
            "üî¥ GHI √ÇM",
            "",
            "ƒêang k·∫øt n·ªëi...",
            "Nh·∫•n n√∫t ƒë·ªÉ d·ª´ng"
        ], (255, 100, 100), (50, 0, 0))
        
        # Start WebSocket in background thread
        ws_thread = threading.Thread(target=start_websocket_thread)
        ws_thread.daemon = True
        ws_thread.start()
    
    else:
        # ===== STOP RECORDING =====
        print("‚èπ Stopping recording...")
        is_recording = False
        stop_streaming = True
        
        # Send flush command to process remaining audio
        # (handled in websocket session)
        
        current_state = State.IDLE
        show_message([
            "ƒê√£ d·ª´ng ghi √¢m",
            "",
            "Nh·∫•n n√∫t ƒë·ªÉ",
            "ghi l·∫°i"
        ], (100, 255, 100))


# ============ MAIN ============
def main():
    global current_state
    
    print("=" * 50)
    print("üé§ REAL-TIME VSL - Raspberry Pi")
    print("=" * 50)
    print(f"üì° Server: {API_URL}")
    print(f"üìπ Videos: {len(video_mapper.video_cache)}")
    print(f"üîò Button: GPIO {BUTTON_PIN}")
    print("=" * 50)
    
    print("Initializing LCD...")
    init_lcd()
    print("‚úÖ LCD OK!")
    
    show_message([
        "Real-Time VSL",
        "",
        "Nh·∫•n n√∫t ƒë·ªÉ",
        "b·∫Øt ƒë·∫ßu"
    ], (100, 255, 100))
    
    last_state = GPIO.HIGH
    
    print("\n‚úÖ Ready! Press button to start...")
    print("   Press Ctrl+C to exit\n")
    
    try:
        while True:
            current_btn = GPIO.input(BUTTON_PIN)
            
            if last_state == GPIO.HIGH and current_btn == GPIO.LOW:
                handle_button()
                time.sleep(0.3)  # Debounce
            
            last_state = current_btn
            time.sleep(0.01)
    
    except KeyboardInterrupt:
        print("\nüëã Exiting...")
        stop_streaming = True
        stop_video = True


if __name__ == "__main__":
    try:
        main()
    finally:
        spi.close()
        GPIO.cleanup()
        print("‚úÖ Cleanup done!")
